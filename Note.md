# TPE ISSD

## gatien CHENU, Emna Kriaa



# 1. Introduction
Les mod√®les de langages sont des mod√®les statistiques qui permettent de pr√©dire la probabilit√© d'apparition d'un mot dans un texte √† partir des mots qui le pr√©c√®dent. Ces mod√®les sont utilis√©s pour la g√©n√©ration de texte, la correction orthographique, la traduction automatique, la d√©tection de la langue d'un texte, etc.
Ils ont subies un fort essor ces derni√®res ann√©es, notamment gr√¢ce √† l'augmentation des donn√©es disponibles et √† l'augmentation des capacit√©s de calcul. Les mod√®les de langages sont des mod√®les de r√©seaux de neurones profonds, qui sont des mod√®les d'apprentissage automatique. Ils sont donc tr√®s sensibles aux donn√©es d'entr√©es, et √† la mani√®re dont elles sont pr√©par√©es.

# les orientations du sujet,
au tout d√©part l'id√©e √©tait d'√©crire un tout petit mod√®le de langages utilisant les √©mojis , ou alors utilisait une mini langue ( langue de jeux vid√©os) pour d√©couvrir et mettre en application notre d√©couverte des mod√®les de langages.

dans un premier temps, nous avons suivi [ce tutoriel](https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0)

## les mod√®les de vectorization entrain√©s

| Nom      | Cr√©ateur | Ann√©e     |
| :---        |    :----:   |          ---: |
| [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf)   |   Google      |    2013   |
|[GloVe](https://nlp.stanford.edu/projects/glove/) |Stanford| 2014|
|[FastText](https://arxiv.org/pdf/1607.04606.pdf)| Facebook| 2015-2018|
|[ELMo](https://arxiv.org/pdf/1802.05365.pdf)| AllenNLP| 2018|

## mod√®le de langange
| Nom      | Cr√©ateur | Ann√©e     |
|[BERT](https://arxiv.org/pdf/1810.04805.pdf)| Google| 2019|
|[GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)| OpenAI| 2018|
|[XLNet](https://arxiv.org/pdf/1906.08237.pdf)| Google| 2019|
|[RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)| Facebook| 2019|
|[GPT-3](https://arxiv.org/pdf/2005.14165.pdf)| OpenAI| 2020|
|[GPT-4](https://arxiv.org/pdf/2303.08774.pdf)| OpenAI| 2021|




GPT : Generative Pre-trained Transformer

papier embl√©matique : [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

mais les Generative Pre-trained model ne sont pas des mod√®les de word2-vec


# 2. Emoji2Vec

##¬†mise en contexte
les emoji sont cod√© au format unicode , et sont donc des caract√®res comme les autres. On peut donc les coder comme des mots, et les utiliser dans un mod√®le de langages. Maleureusement les emoji sont tr√®s peu utilis√© dans les datasets de textes. Nous avons cherch√© les endroits o√π pouvait √™tre le plus utiliser les √©mojis, c'est √©videmment dans les messages priv√©s et les r√©seaux sociaux. 
malheureusement les messages priv√©s sont garder pr√©cieusement et sur les r√©seaux sociaux l'utilisation des √©mojis est tr√®s diff√©rente de l'utilisation des mots. 
les √©mojis ne sont pas utilis√© comme des mots, mais plut√¥t comme des ponctuations.

Nous avons eu l'id√©e de r√©aliser la vectorization de chaque √©moji en utilisant leur description. 
La publication emoji2vec a bien entra√Æn√© seulement sur la description des emojis , ce qui est un √©norme bien compar√© √† leur utilisation r√©elle.
Exemple : chez les jeunes : üò≠ ou encore üíÄ 
Peut repr√©senter :" mort de rire " ce qui n'est pas le cas dans un mod√®le avec seulement les descriptions pour l'utilisation "initiale" des emojis , selon emoji2vec, 10 % des tweets comprennent un emoji
De plus l'utilisation des emojis sur twitter diff√©rent des autres utilisations sur d'autres services.

#¬†note et pens√©

dans une IA de compl√©tion de texte , les tokens peuvent √™tre des mots, des caract√®res, des syllabes, des √©mojis, etc. 
il est important aussi qu'elle ne puisse √™tre que des fractions de mots, car cela permet de g√©n√©rer des mots inconnus: 
[√† 9 min ](https://www.youtube.com/watch?v=Sv5OLj2nVAQ)

d'o√π viennent les mots inconnus ? peuvent-t'il venir d'autre part que le prompt de d√©part ? est-ce que ce serait utile de cr√©er des tokens temporaires pour les mots inconnus ? 

## - le pr√©-traitement des donn√©es 
je remarque sur le [word2vec fran√ßais](http://nlp.polytechnique.fr/word2vec) qu'il y a beaucoup de mot mal trait√© et qui aurait d√ª √™tre regroup√© en un seul token: 
- exemple: @google -> google

ou plus dur:
 - twttr-> twitter

il y a aussi des liens dans le dataset 
- exemple : http://wordpress.org/?v=3.5

il y a aussi les mots tel que : adblock, adblockers, adblocks 

il faudrait trouver un moyen de les regrouper en un seul token, ou alors de les traiter diff√©remment.

alors pour traiter au mieux ses donn√©es on peut tout d'abord r√©aliser de l'exploration , par exemple afficher les donn√©es contenant des caract√®res sp√©ciaux


# dataset 
Nous avons donc cherch√© un dataset de tweets, et nous avons trouv√© [celui-ci](https://www.kaggle.com/kazanova/sentiment140) qui contient 1,6 millions de tweets.

#¬†bibliographie:

- page wikip√©dia des √©mojis: https://fr.wikipedia.org/wiki/Emoji

- Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf). In Proceedings of Workshop at ICLR, 2013.

- Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/pdf/1310.4546.pdf). In Proceedings of NIPS, 2013.

- Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. [Linguistic Regularities in Continuous Space Word Representations](https://aclanthology.org/N13-1090.pdf). In Proceedings of NAACL HLT, 2013.

- Ben Eisner, Tim Rockt√§schel, Isabelle Augenstein, Matko Bo≈°njak, Sebastian Riedel [emoji2vec: Learning emoji representations from their description](https://arxiv.org/pdf/1609.08359.pdf) (2016)


https://www.mdpi.com/2078-2489/11/1/24
papier sur la pertincence de la ponctuation dans les mod√®les de langages:
int√©ressant car on peut voir  les √©mojis comme de la ponctuation


Attention Is All You Need 
papier sur l'arriv√©e des transformer mais c'est pas vraiment dans le contexte de notre TPE , car cela permet de mettre en contexte dans une phrase

