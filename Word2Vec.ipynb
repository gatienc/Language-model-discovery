{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚠️ ce fichier content les premières réfléxions sur l'article de towarsdsdatascience  \n",
    "Nous avons suite utiliser le word2vec d'Olga Chernytska\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "EMBED_DIMENSION = 300 \n",
    "EMBED_MAX_NORM = 1 \n",
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(CBOW_Model, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM,\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW_Model  hérite de nn.Module c'est la classe de base pour tous les modules de réseaux de neurones dans PyTorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la dimension des embeddings est le nombre de caractéristiques ou de dimensions dans lesquelles chaque mot est représenté. Par exemple, si la dimension des embeddings est de 100, chaque mot du vocabulaire sera représenté par un vecteur de 100 éléments (dimensions).\n",
    "\n",
    "Une dimension d'embedding plus élevée permet de capturer plus d'informations et de nuances sur les relations entre les mots, mais augmente également la complexité du modèle et le temps d'entraînement. Une dimension trop faible peut, en revanche, conduire à une perte d'informations et à de moins bonnes performances lors de l'utilisation des embeddings pour des tâches de traitement du langage naturel (NLP).\n",
    "\n",
    "En pratique, la dimension des embeddings est souvent choisie en fonction de la taille du vocabulaire et des performances souhaitées pour les tâches NLP spécifiques. Les valeurs courantes pour la dimension des embeddings varient généralement entre 50 et 300, bien que des valeurs plus élevées puissent être utilisées pour des ensembles de données plus importants ou des tâches plus complexes.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le paramètres max_norm est le choix de la norme utilisé pour la normalisation du vecteur d'embedding :\n",
    "norme L1 : max ?\n",
    "\n",
    "\n",
    "L'utilisation de la norme maximale pour les vecteurs d'embedding présente plusieurs avantages :\n",
    "\n",
    "    Stabilité numérique : La normalisation des embeddings peut améliorer la stabilité numérique de l'apprentissage et éviter des problèmes tels que l'explosion ou la disparition des gradients.\n",
    "    Régularisation : La contrainte de norme agit également comme une forme de régularisation, ce qui peut aider à prévenir le surapprentissage et améliorer la généralisation du modèle.\n",
    "    Comparabilité : Des embeddings normalisés sont plus faciles à comparer entre eux, car ils se situent sur une échelle commune. Cela peut améliorer les performances du modèle pour certaines tâches, telles que la recherche de similarités entre les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.eval()  # Mettre le modèle en mode évaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " il est important de basculer en mode évaluation avant d'effectuer des prédictions ou des évaluations.\n",
    "\n",
    "Le mode évaluation informe le modèle qu'il ne sera pas utilisé pour l'entraînement et que certaines opérations spécifiques à l'entraînement, telles que le dropout ou la normalisation par lots (batch normalization), doivent être désactivées ou modifiées en conséquence. Cela garantit que le modèle effectue des prédictions correctes lorsqu'il est utilisé pour l'inférence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
